# LoRA-from-Scratch-Low-Rank-Adaptation-of-GPT-2LoRA-from-Scratch-Low-Rank-Adaptation-of-GPT-2
This project implements LoRA (Low-Rank Adaptation) from scratch using only PyTorch â€” no external LoRA libraries. It fine-tunes GPT-2 (124M parameters) on Shakespeare's works, teaching the model to generate Shakespearean text by training only ~0.15% of the total parameters.
